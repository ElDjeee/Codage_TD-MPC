# Compte-rendu du 02/02/2024

## Priority Loss :
- Utilisé pour trier les échantillons du replay buffer.
- Prioritized Experience Replay aide à déterminer les pertes les plus importantes en triant les échantillons par la quantité d'erreur de différence temporelle.

## Utilisation de Replay Buffer :
- Souvent utilisé avec une taille infinie pour stocker des données.
- L'action est décidée en regardant l'observation actuelle, ce qui crée une boucle fermée (politique).
- Un réseau neuronal généraliste est utilisé, pouvant être formé sur différentes trajectoires.
- Les modèles basés sur des boucles ouvertes ne vérifient pas leurs prédictions avec l'environnement réel.

## Modèles en RL :
- Les modèles sont souvent légèrement incorrects.
- Auparavant, le modèle-free fonctionnait mieux que le modèle basé, mais maintenant, TD-MPC fonctionne mieux que le modèle-free.
- Affirmation fausse : "Firstly, we learn..."

## Interaction entre l'Agent et l'Environnement :
- L'environnement envoie une image à l'agent, qui la transforme en une variable latente.
- Les différentes pertes servent à apprendre le réseau neuronal.
- L'agent collecte des interactions avec l'environnement.

## Autres Points :
- Z_t contient les informations nécessaires.
- L'agent prédit la prochaine image.
- Utilisation d'une norme au carré de 2,2.
- Replay buffer stocke des données pour une réutilisation multiple, triées par importance.
- Option de lissage ajoutée en RL pour éviter...
- Interaction entre la Q table et la politique dans l'actor-critic.
- TD-MPC = TD3 + modèle basé, où le modèle basé est utilisé pour entraîner TD3.
- Dynamic Programming and Optimal Control mentionnés comme références.
- Cross-Entropy Method (CEN) utilisé dans le modèle prédictif de commande (MPC).
- MPC utilise la planification en horizon glissant et le réapprentissage.
- Apprentissage auto-supervisé mentionné.
- Utilisation de softmax pour la netteté (sharpness).
- MPC est entre la boucle ouverte et fermée.
- Warm start : utilisation de l'information précédente pour accélérer le processus.
- DDPG est considéré comme l'ancêtre de TD3.
- Linear Annealing utilisé pour décroître linéairement la variance.
- Augmentation de l'horizon H au fil du temps.
- Critiques formulées sur l'algorithme.

## TODO
- Examiner le code GitHub correspondant à chaque section de l'article.

## Nos Disponibilités
- Mercredi de 13h45 à 15h45.
- Tous les jours à partir de 18h.
