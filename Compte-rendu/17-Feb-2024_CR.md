# Compte-rendu du 17/02/2024

## Retour sur les slides :
- Les pensées ne sont pas assez organisées/structurées.
- Il va falloir refaire la présentation, en précisant chaque bout, expliquer leur fonctionnement, décomposer les concepts...
  * Tout n'est pas correctement décomposable (e.g. réseau de neurones).
- Il va falloir comprendre plus en détail les notions présentées.
- Ne plus utiliser les ressources disponibles sur le site de TD-MPC.
- D'ici la fin, il va falloir connecter ces notions avec BBRL.

## Explication BBRL :
### Infos supplémentaires :
- Tout est agent !
- On va pouvoir décomposer un agent en plusieurs agents.
  * TAgent peut être composé en chaîne : EnvAgent => ProbaAgent => ActionAgent.
  * "Donne une obs", "Donne les probabilités des actions", "Donne l'action".
  * On dit que TAgent encapsule EnvAgent, ProbaAgent, ActionAgent.
- Chaque agent a une méthode forward.
### Pour le projet :
- Nous cherchons à exploiter au mieux les propriétés de BBRL pour l'implémentation du projet.
- On va vouloir faire de la prédiction d'images.
  * Par exemple, dans Cartpole, [X, X_point, theta, theta_point] suffit pour avoir l'état complet du système.
  * Mais nous ne cherchons pas directement à avoir cet état, mais l'image suivante !
- Un étudiant s'est déjà occupé de la gestion d'image pour BBRL (quelques fixs sont à prévoir).
- ParallelGymAgent est un EnvAgent. Mais il va falloir le remplacer par VisualGymAgent pour gérer les images.
- L'image, avant d'être ajoutée au workspace, doit être préprocessée afin de réduire les coûts de calcul.
- Un agent fera le prétraitement (code de l'étudiant).
  * RGB => Niveau de gris.
  * réduction de la taille de l'image.
- Plusieurs images peuvent être nécessaires pour prédire la suivante.
  * Dans le cas de Cartpole, la vitesse du chariot et du pendule ne peut être prédite avec une seule image.
  * VisualSerialGymAgent permet de gérer plusieurs images (code de l'étudiant).

## Cours DQN :
- Utilisation de réseau de neurones dans le cadre de l'apprentissage.
- RL avec l'approximation de fonction peut diverger !!!
- Parallélisation possible (notamment avec les GPU).
- Entrée continue, sortie discrète (nombre de sorties = nombres d'actions possibles).
- Le Q-network équivaut à la Q-table (avec une infinité d'états possibles).
- On minimise l'erreur de prédiction de récompense.
- Fitted-Q vient en remplacement de Deep QN et les paramètres theta mettent à jour les paramètres des réseaux neuronaux.
- Quand on change un paramètre, toutes les cibles peuvent bouger. Solutions :
  * On va agir de manière incrémentale sur les modifications.
  * On va copier la target network, et on va agir dessus. Mais on va continuer de mesurer l'erreur sur l'ancienne. Les nouveaux paramètres seront mis à jour toutes les K itérations.
- Pour le réseau cible, voir 12/24 DQN.

## Conclusion
### Infos :
- Il va falloir itérer le plus possible sur la présentation afin de l'affiner.
- Des notebooks sont à faire pour appréhender les propriétés de BBRL.

### À faire :

Ce week-end : nous travaillons sur toutes les ressources que vous avez envoyées.

Lundi : Mise en commun de tout ce que nous avons compris + division du travail (un groupe pour gérer l'interaction avec les images et le deuxième pour commencer le code TOLD avec BBRL).

Reste de la semaine : chaque groupe travaille sur sa partie.

Dimanche 25 février : mise en commun du travail de chaque groupe + refaire l'exposé.
