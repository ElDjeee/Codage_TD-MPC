# Ordre du jour - 31/01/2024

## Ce que nous avons fait :

- Nous avons fait le point sur les étapes à suivre pour le projet.
- Nous avons essayé de faire fonctionner le code sur les machines de la PPTI, sans succès car nous n'avons pas pu accéder à la salle.
- Nous avons commencé à examiner le code sur GitHub.
- Nous avons consulté la documentation de BBRL.

## Questions sur le code :

- S'assurer d'avoir bien compris la "priority loss" de l'implémentation de TD-MPC.

## Ce que nous aimerions traiter lors de la prochaine séance :

- S'assurer que nous avons compris les principaux aspects du sujet.
- L'implémentation de BBRL.
- L'implémentation du code.

# Edit du 01/02/2024

## Liste de questions :

“using a learned model to improve sample-efficiency of model-free methods by e.g. learning from generated rollouts, but this makes model biases likely to propagate to the policy as well (Ha & Schmidhuber, 2018);” Comment/Pourquoi ?

 “Hafner et al., 2020b; Clavera et al., 2020). As a result, model-based methods have historically struggled to outperform simpler, model-free methods (Srinivas et al., 2020; Kostrikov et al., 2020) in continuous control tasks.” Pourquoi ?

“Firstly, we learn the latent representation of the dynamics model purely from rewards.” Comment ?

“Secondly, we back-propagate gradients from the reward and TD-objective through multiple rollout steps of the model” Through multiple rollout steps of the model ?

“modality-agnostic prediction loss in latent space that enforces temporal consistency in the learned representation without explicit state or image prediction” modality-agnostic ? Comment temporal consistency fonctionne ?

Equation (1): Pourquoi les 2 2 ?

Est-ce que le replay buffer sert uniquement à stocker d’anciens épisodes, et les trier par l’importance ?

“θ− is a slow-moving average of the online parameters θ updated with the rule …” Qu’est-ce que c’est ?

“actor-critic RL” comment cela fonctionne ?

“Equation 2 can be viewed as a special case of the standard additive-cost optimal control objective.” Besoin d’éclairage sur cette phrase

“derivative-free Cross-Entropy Method (CEM; Rubinstein (1997))” Comment cela fonctionne ?

“Model Predictive Path Integral” une explication rapide serait la bienvenue.

Equation 3: “γHQ (z ,a )”, c’est parce qu’on ne sait pas le reward, donc on se contente de sa value ?

Comment d_{\theta} fonctionne ?

sharpness de \Omega_{I}, qu’est-ce que ça signifie exactement ?

“we employ receding-horizon MPC to produce a feedback policy.” La phrase précédente dans l'article suffit peut-être, mais nous voulons bien une brève explication du terme "receding-horizon" et "feedback policy", histoire d’être sûr.

“warm start”, “reusing the 1-step shifted mean μ obtained at the previous step” qu’est-ce que cela signifie?

“Model-free RL algorithms such as DDPG (Lillicrap et al., 2016) encourage exploration by injecting action noise (e.g. Gaussian or Ornstein-Uhlenbeck noise) into the learned policy πθ during training, optionally following a linear annealing schedule.” que veut dire exactement “linear annealing schedule” ?

“Likewise, we linearly increase the planning horizon from 1 to H in the early stages of training, as the model is initially inaccurate and planning would therefore be dominated by model bias.” qu’est-ce que cela veut dire?

“TD-MPC learns a policy π_θ in addition to planning procedure Π_θ, and augments the sampling procedure with additional samples from πθ (highlighted in blue in Algorithm 1).” On mélange des samples réels avec des samples générés ?
