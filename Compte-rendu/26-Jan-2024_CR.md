# Compte-rendu du 26/01/2024

## Perception de l'environnement et traitement des données
- L'agent agît comme le cerveau du robot (ex: Humanoid, Dog, ...), le dirigeant. L'agent perçoit son env et maximise les récompenses en faisant les meilleurs actions.
- Perception de l'env peut se faire par une/des img à chaque instant, et/ou des infos sur le robot et ce qui l'entoure (ex: position, vitesse, ...), cette perception représente l'état.
- Ensemble des états => Espace d'états.
- Img trop lourdes => les encoder en vecteurs latents (zt), servant ainsi d'état.
- *Latent dynamics* : système de l'espace latent.
- Les réseaux de neurones peuvent servir à traiter les entrées.
- Dans notre cas, nous avons une img de l'env, et nous voulons apprendre des img que nous percevons.

## Mécanismes d'apprentissage et d'optimisation
- Les neurones vont aider à la décision et corrige, avec les récompenses obtenues par l'agent.
- Comment choisit-il les données ? C'est le cœur du projet.
- Le réseau apprend une représentation efficace, focalisant sur les données importantes pour contrôler le système.
- Une loss function est définie afin d'apprendre et ajuster les gradients.
- Apprentisage de représentation pertinente => representation learning.
- Aide à résoudre la malédiction de la dimensionnalité.
- Prédiction de la dynamique latente pour connaître l'état du système (ex. position et vitesse d'une boule).
- Modèle TOLD (cœur du projet) est centré sur la façon dont nous apprenons la représentation, avec une réduction de dimension qui s'applique. (cf. Partie 4 de l'article)
- Multi-task : l'agent reçoit un état et une tâche, les agents sont conditionnés par une tâche, et ils apprennent à faire plusieurs tâches à la fois.
- Travail dirigé (TD) sur les mécanismes de renforcement : calcul de la fonction de valeur d'action.
- Contrôle prédictif de modèle (MPC) pour prédire les états futurs du robot.
- Intégration du MPC avec le TD pour améliorer les prédictions.
- Apprentissage d'un modèle forward pour prédire les actions du robot.
- Apprendre le modèle du robot (à faire) : régression, des fonctions qui donnent l'état suivant du robot en fonction de son état.
- Amélioration du modèle pour éviter l'apprentissage d'informations incorrectes.
- Cross-entropy : utilisée pour optimiser la séquence d'actions.
- PETs est également mentionné (Probabilistic Ensembles with Trajectory Sampling).
- La différence entre le modèle du robot et le modèle de l'env réside dans la perspective de l'agent, où le robot est à la fois l'agent et une partie de l'env, et le système (dynamics) est composé du robot et de l'env.
- Prédiction des séquences latentes et des récompenses.


## Infrastructure et outils
- `gym.make(env)` pour définir l'env.
- Création de wrappers pour ajouter des fonctionnalités à l'env de gym.
- Callbacks pour évaluer l'agent pendant l'entraînement. 
- Au lieu d'interagir directement avec l'env, l'agent peut interagir avec wrapper, et peut step et reset pour modéliser la dynamique du système.
- Infos utiles:
    - loss.backward en PyTorch, rétropropage l'erreur sur le réseau de neurones, utilisant le gradient pour diminuer l'erreur.
    - Les tâches que nous voulons utiliser dans notre travail sont toutes fournies dans le Gym.
    - SB3 => bibliothèque.
    - Tâche : envs disponibles sur internet, MethaWorld.
    - La boucle d'interaction centrale de tous les algorithmes RL est la gym interaction loop, où l'agent prédit les actions, l'env prédit l'état suivant.
    - En RL, deux choses : entraîner l'agent et évaluer l'agent.

## TODO
- Relire les implémentations et relire l'article en parallèle pour comprendre.
- Lister les installations à faire à la PPTI
- Visionner des vidéos YouTube, notamment BBRL.

## Recommandations pour la suite
- Commencer le code
- M. Sigaud nous envoie des TP de codage de l'algorithme de RL pour regarder.

*Rappel* : La partie TOLD est la plus importante, le pipeline de prétraitement réduit la taille de l'img pour RL. (Task-Oriented Latent Dynamics)
