# Compte-rendu du 10/02/2024

1ère partie : BBRL.
2nd : questions.
Puis, TODO et Direction.

## Cours sur BBRL : [BBRL Foundations](https://master-dac.isir.upmc.fr/rl/bbrl.pdf)

BBRL veut dire **BlackBoard RL**.

### Wrapper / Callback
Un wrapper permet, entre autres, d'encapsuler l'environnement (Décorateur) spécifique.  
Un callback est une fonction invoquée à un moment précis dans un processus (Visiteur).

On a un agent qui interagit avec un wrapper qui est un env gym. L'agent a un attribut callback. Un callback peut être une liste de callbacks dans la boucle -> permet de composer des comportements.

### Interaction/Architecture de BBRL
ActionEnv écrit l'action, EnvAgent écrit le reste.  
Le workspace est le blackboard. Il est partagé pour tous les agents, qui écrivent et lisent dessus (temporal data).  
Les agents sont des nn.Modules, parfaits pour paralléliser.

On sépare toujours les données pour l'entraînement et les données pour l'évaluation de l'algorithme.  
Évaluer la politique à chaque pas de temps n'étant pas une bonne idée, on fixe une fréquence d'évaluation (en nombre d'étapes).  
On évalue la politique trouvée sur un nombre n d'épisodes (qui sont parallélisables).

Buffer : dans TD3 il y a un mécanisme de ReplayBuffer, stockant tous les échantillons d'apprentissage. Il y a différents types de buffers.

Limitation de SB3 : librairie extrêmement utilisée, on l'utilise par défaut quand on veut utiliser du RL parce que c'est simple à utiliser, mais pas du tout fait pour l'enseigner aux étudiants. La librairie BBRL est plus légère et simple.

Avec BBRL, il n'y a pas de boucle centrale.

### Workspace
Le concept de blackboard est utilisé dans les méthodes de multi-agents, où plusieurs agents interagissent en lisant et en écrivant sur un espace de travail partagé, appelé le workspace.

Chaque agent contribue en écrivant des observations et des récompenses dans ce workspace, tandis que d'autres agents lisent ces informations.  
Le workspace est un dictionnaire contenant des clés représentant les variables écrites par les agents, et des tenseurs stockant des séquences de données au fil du temps.

#### Autoreset = false
Lorsqu'un épisode se termine pour un des agents, l'agent ne va pas reset.  
Il va se contenter de recopier la dernière étape de son épisode, à chaque step, et ce jusqu'à ce que tous les autres agents aient fini leurs épisodes.  
Ainsi, à la fin de l'épisode, il suffit de regarder la dernière étape de chaque environnement pour obtenir une image complète de la fin de l'épisode pour tous les environnements.

#### Autoreset = true
Lorsqu'un épisode se termine pour un des agents, l'agent va reset.  
Pour savoir quand s'arrêter, on fixe un epoch (une durée en pas de temps), créant ainsi un bloc.  
Pour chaque début d'epoch (sauf le premier), on recopie la dernière étape du bloc précédent afin de ne pas perdre les transitions.  
Pour éviter l'apprentissage de données inter-épisodes (ce qui n'a pas de sens), on va supprimer la transition menant au reset.

La slide 16/21 l'illustre, avec la step2-step3 qui est inter-épisode.  
En pratique, on appellera ```workspace.get_transitions()```, qui se chargera de faire tout le travail.

## Réponses aux questions de l'ODJ du 07/02/2024

1. ”TD-MPC and TOLD support continuous action spaces, arbitrary input modalities, and sparse reward signals.” arbitrary input modalities ?   Modalité d'entrée arbitraire, c'est-à-dire que TD-MPC et TODL fonctionnent quelle que soit la modalité (image, son, texte, etc.).

2. (Equation 10). On suppose donc que l’état ne change pas trop d’un pas de temps à un autre. Marche forcément en temps continue ? Quid en temps discret ?  C'est forcément un temps discret, nous sommes en informatique. L'état ne devra pas changer trop vite. On punit si l'état latent change trop vite, il faut que l'algo tourne assez vite pour que l'environnement ne puisse pas changer plus vite.

3. “and gradients from all three terms are back-propagated through time. “ Trois ? Qui sont-ils ?  c1, c2, c3 (Cf. Equation 10)

4. “We use an exponential moving average θ−”, historique de combien ?  cf. code source

5. R.A.S
  
6. “Here, sg denotes the stop-grad operator, and Equation 11 is optimized only wrt. policy parameters.” sg revient à annuler l’état latent pendant la backdrop, wrt ?  Stop gradient. PyTorch mémorise tous les calculs faits, aidant à la rétropropagation. On peut avoir besoin de ne pas prendre certains termes en compte lors du backward, sg sert à cela.

7. “goal-conditioning” ?  Il apprend à faire la bonne action pour son but (ses buts en pratique). Et on espère pouvoir généraliser.
   
8. sparse rewards => pas de reward à chaque instant ?  Oui (C'est un gros problème pour l'apprentisage, les goals peuvent aider à résoudre cela).
   
9. “Are TOLD models capable of multi-task and transfer behaviors despite using a reward-centric objective?” Transfer behaviors ? Reward-centric ?  Avec la même politique on peut résoudre plusieurs problèmes. Et on peut transférer la connaissance d’une tâche à l’autre (pas tout le temps !).

## TODO/Direction
- Faire une présentation de notre projet, permettant de mettre nos idées au clair, et vérifier si l'on a bien compris les tenants et les aboutissants.
- (Prendre en main BBRL avec les deux notebooks reçus par mail.)

La présentation permettra de faire un point de contrôle pour ce projet. On s'assurera d'avoir compris le sujet, afin de pouvoir commencer à parler concrètement de l'implémentation de TD-MPC avec BBRL.  
Le second TODO est en parenthèses pour deux raisons. Nous ne sommes pas sûrs d'avoir le temps d'ici le 14/02/2024. Et la priorité est de s'assurer d'avoir "maîtrisé" la théorie derrière TD-MPC.
Une fois la présentation faite, il nous faudra décortiquer le code de TD-MPC et commencer à imaginer comment faire le transfert à BBRL.
